{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from video_process_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processed/all_videos_dict.pickle', 'rb') as handle:\n",
    "    all_videos = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata_processed =\\\n",
    "    pd.read_csv(\"./data/processed/alldata_processed.csv\")\n",
    "alldata_processed = alldata_processed.groupby(['videoid'],as_index=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_USED = [LANK,RANK,LKNE,RKNE,LHIP,RHIP,LBTO,RBTO] #only these columns factor in the calculation of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "vid_length = 124\n",
    "for videoid, raw_video in all_videos.items():\n",
    "    if len(raw_video.shape) == 2 and len(raw_video) >= vid_length:\n",
    "        raw_video = drop_confidence_cols(raw_video[:500,:].copy())\n",
    "        #for each column, compute the % of values missing\n",
    "        #then, take the max of those missing values\n",
    "        pct_missing = max_pct_nan_or_zero_given_cols(raw_video,COLS_USED)\n",
    "        \n",
    "        n_segments = 0\n",
    "        start_idx = 0\n",
    "        for i in range(start_idx,500-vid_length,31):\n",
    "            raw_video_chunk = raw_video[i:i+vid_length,:]\n",
    "            pct_missing_chunk =\\\n",
    "                max_pct_nan_or_zero_given_cols(raw_video_chunk,COLS_USED)\n",
    "            if pct_missing_chunk <= 0.25 and len(raw_video_chunk) == vid_length:\n",
    "                n_segments += 1\n",
    "        \n",
    "        result.append([videoid,pct_missing,n_segments])\n",
    "result = np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array(result),columns=['videoid','pct_missing','n_segments'])\n",
    "df = df[(df['pct_missing'] <= 0.25)]\n",
    "df = df.merge(right=alldata_processed[['videoid','Patient_ID']],on=['videoid'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each patient_ID, assign a random number\n",
    "np.random.seed(1)\n",
    "rand_df = pd.DataFrame(alldata_processed['Patient_ID'].unique(),columns=['Patient_ID'])\n",
    "rand_df[\"random_num\"] = np.random.uniform(0,1,len(rand_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(right=rand_df,on=['Patient_ID'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80/10/10 split (so roughly 200 in each of validation/test)\n",
    "def assign_split(x):\n",
    "    if x <= 0.80:\n",
    "        return \"train\"\n",
    "    elif x > 0.80 and x <= 0.9:\n",
    "        return \"validation\"\n",
    "    else:\n",
    "        return \"test\"\n",
    "df['dataset'] = df['random_num'].apply(assign_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(df.groupby('Patient_ID')['dataset'].nunique().max()==1) #verify that videos from each Patient_ID appear in only one split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/processed/train_test_valid_id_split.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('dataset')['n_segments'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df['n_segments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('dataset')['n_segments'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
